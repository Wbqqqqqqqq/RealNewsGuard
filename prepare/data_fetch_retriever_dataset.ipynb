{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T04:51:55.332818Z",
     "iopub.status.busy": "2025-02-08T04:51:55.332431Z",
     "iopub.status.idle": "2025-02-08T04:51:56.579560Z",
     "shell.execute_reply": "2025-02-08T04:51:56.578367Z",
     "shell.execute_reply.started": "2025-02-08T04:51:55.332783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
    "params = {\n",
    "    \"q\": \"education\",\n",
    "    \"api-key\": # Your API KEY\n",
    "    \"page\": 0,\n",
    "    \"pageSize\": 10\n",
    "}\n",
    "\n",
    "response = requests.get(api_url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    articles = data.get(\"response\", {}).get(\"docs\", [])\n",
    "    print(f\"找到 {len(articles)} 篇文章\")\n",
    "    for article in articles:\n",
    "        print(article.get(\"web_url\"))\n",
    "else:\n",
    "    print(f\"请求失败，状态码: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch News Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T08:32:57.381287Z",
     "iopub.status.busy": "2025-02-09T08:32:57.380813Z",
     "iopub.status.idle": "2025-02-09T08:33:26.092584Z",
     "shell.execute_reply": "2025-02-09T08:33:26.091153Z",
     "shell.execute_reply.started": "2025-02-09T08:32:57.381253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install news-please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T07:33:30.195191Z",
     "iopub.status.busy": "2025-02-09T07:33:30.194839Z",
     "iopub.status.idle": "2025-02-09T07:33:30.207724Z",
     "shell.execute_reply": "2025-02-09T07:33:30.206354Z",
     "shell.execute_reply.started": "2025-02-09T07:33:30.195147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from newsplease import NewsPlease\n",
    "\n",
    "API_KEY = # Your API KEY\n",
    "BASE_URL = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
    "\n",
    "def fetch_news(query=\"education\", total_articles=20, max_retries=8):\n",
    "    \"\"\"\n",
    "    Fetches news articles from the New York Times API based on the search query.\n",
    "    \n",
    "    For each page (index), if the request fails it will retry up to max_retries times.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search keyword.\n",
    "        total_articles (int): Total number of articles to fetch.\n",
    "        max_retries (int): Maximum number of retries per page (index).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dataset containing articles with fields such as 'id', 'title', 'url', 'section', 'content'.\n",
    "    \"\"\"\n",
    "    dataset = {\n",
    "        \"id\": [],\n",
    "        \"title\": [],\n",
    "        \"url\": [],\n",
    "        \"section\": [],\n",
    "        \"content\": []\n",
    "    }\n",
    "    \n",
    "    current_articles = 0\n",
    "    current_page = 0\n",
    "    articles_per_page = 10  # NYTimes API returns up to 10 articles per page\n",
    "\n",
    "    while current_articles < total_articles:\n",
    "        # 对于当前页，初始化单页重试计数器\n",
    "        page_retry = 0\n",
    "        articles = None\n",
    "\n",
    "        # 尝试请求当前页，失败则重试（针对该页统计重试次数）\n",
    "        while page_retry < max_retries:\n",
    "            params = {\n",
    "                \"q\": query,\n",
    "                \"api-key\": API_KEY,\n",
    "                \"page\": current_page,\n",
    "            }\n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                articles = data.get(\"response\", {}).get(\"docs\", [])\n",
    "                # 成功获取数据，退出重试循环\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                page_retry += 1\n",
    "                print(f\"⚠️ Request failed for page {current_page} (retry {page_retry}/{max_retries}): {e}\")\n",
    "                time.sleep(200)  # 暂停一段时间后重试\n",
    "            except ValueError as e:\n",
    "                page_retry += 1\n",
    "                print(f\"⚠️ JSON parsing failed for page {current_page} (retry {page_retry}/{max_retries}): {e}\")\n",
    "                time.sleep(200)\n",
    "\n",
    "        if page_retry == max_retries:\n",
    "            print(f\"❌ Failed to fetch page {current_page} after {max_retries} retries.\")\n",
    "            break\n",
    "\n",
    "        if not articles:\n",
    "            print(f\"❌ No more articles found on page {current_page}.\")\n",
    "            break\n",
    "\n",
    "        # 遍历当前页中的所有文章\n",
    "        for article in articles:\n",
    "            dataset[\"id\"].append(article.get(\"_id\"))\n",
    "            dataset[\"title\"].append(article.get(\"headline\", {}).get(\"main\", \"No Title\"))\n",
    "            url = article.get(\"web_url\", \"No URL\")\n",
    "            dataset[\"url\"].append(url)\n",
    "            dataset[\"section\"].append(article.get(\"section_name\", \"Unknown\"))\n",
    "            \n",
    "            # 尝试抓取文章内容\n",
    "            try:\n",
    "                parsed_article = NewsPlease.from_url(url)\n",
    "                content = getattr(parsed_article, \"maintext\", \"No content available\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to fetch content from {url}: {e}\")\n",
    "                content = \"Failed to fetch content.\"\n",
    "            \n",
    "            dataset[\"content\"].append(content)\n",
    "            \n",
    "            current_articles += 1\n",
    "            if current_articles >= total_articles:\n",
    "                break\n",
    "\n",
    "        print(f\"✅ Fetched {len(articles)} articles from page {current_page}.\")\n",
    "        current_page += 1\n",
    "        time.sleep(10)  # 避免请求过于频繁\n",
    "\n",
    "    print(f\"✅ Total fetched articles: {current_articles}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T07:33:37.538641Z",
     "iopub.status.busy": "2025-02-09T07:33:37.538273Z",
     "iopub.status.idle": "2025-02-09T07:33:48.091345Z",
     "shell.execute_reply": "2025-02-09T07:33:48.089850Z",
     "shell.execute_reply.started": "2025-02-09T07:33:37.538611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# technology, education, business, environment, economy, sports, health, medical\n",
    "# science, international, national, entertainment, society, culture, law, food, space\n",
    "# internet, fashion\n",
    "queries = [\"society\", \"culture\", \"law\", \"food\", \"space\", \"internet\", \"fashion\"]\n",
    "for query in queries:\n",
    "# query = \"economy\"\n",
    "    news_dataset = fetch_news(query=query, total_articles=1000)\n",
    "    \n",
    "    df = pd.DataFrame(news_dataset)\n",
    "    df.to_csv(f\"{query}_news_data.csv\", index=False)\n",
    "    \n",
    "    print(f\"✅ Dataset Saved to {query}_news_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T08:08:33.993443Z",
     "iopub.status.busy": "2025-02-09T08:08:33.992981Z",
     "iopub.status.idle": "2025-02-09T08:08:34.000186Z",
     "shell.execute_reply": "2025-02-09T08:08:33.999026Z",
     "shell.execute_reply.started": "2025-02-09T08:08:33.993407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(min(5, len(news_dataset[\"content\"]))):\n",
    "    print(f\"{i+1}. {news_dataset['title'][i]}\\n{news_dataset['content'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T08:34:15.711495Z",
     "iopub.status.busy": "2025-02-09T08:34:15.711086Z",
     "iopub.status.idle": "2025-02-09T08:34:16.043169Z",
     "shell.execute_reply": "2025-02-09T08:34:16.041900Z",
     "shell.execute_reply.started": "2025-02-09T08:34:15.711463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"/kaggle/input/rag-dataset\"\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        section_name = os.path.splitext(file)[0]\n",
    "        \n",
    "        df[\"section\"] = section_name\n",
    "        \n",
    "        dfs.append(df)\n",
    "\n",
    "if dfs:\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    output_path = os.path.join(\"/kaggle/working/\", \"New_York_Times.csv\")\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"合并后的文件已保存至: {output_path}\")\n",
    "else:\n",
    "    print(\"未找到 CSV 文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T08:34:16.968471Z",
     "iopub.status.busy": "2025-02-09T08:34:16.968054Z",
     "iopub.status.idle": "2025-02-09T08:34:16.983996Z",
     "shell.execute_reply": "2025-02-09T08:34:16.982200Z",
     "shell.execute_reply.started": "2025-02-09T08:34:16.968443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(merged_df.shape)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T08:34:18.919774Z",
     "iopub.status.busy": "2025-02-09T08:34:18.919351Z",
     "iopub.status.idle": "2025-02-09T08:34:18.927637Z",
     "shell.execute_reply": "2025-02-09T08:34:18.926177Z",
     "shell.execute_reply.started": "2025-02-09T08:34:18.919740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(merged_df[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the Error Fetching Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T01:01:14.658241Z",
     "iopub.status.busy": "2025-02-11T01:01:14.657790Z",
     "iopub.status.idle": "2025-02-11T01:01:19.329493Z",
     "shell.execute_reply": "2025-02-11T01:01:19.327971Z",
     "shell.execute_reply.started": "2025-02-11T01:01:14.658206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install lxml_html_clean\n",
    "!pip install --no-cache-dir newspaper3k==0.2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T01:01:19.332236Z",
     "iopub.status.busy": "2025-02-11T01:01:19.331685Z",
     "iopub.status.idle": "2025-02-11T01:01:23.735079Z",
     "shell.execute_reply": "2025-02-11T01:01:23.733510Z",
     "shell.execute_reply.started": "2025-02-11T01:01:19.332134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T02:29:35.426259Z",
     "iopub.status.busy": "2025-02-11T02:29:35.425834Z",
     "iopub.status.idle": "2025-02-11T02:29:36.955410Z",
     "shell.execute_reply": "2025-02-11T02:29:36.954241Z",
     "shell.execute_reply.started": "2025-02-11T02:29:35.426228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from newspaper import Article\n",
    "\n",
    "folder_path = \"/kaggle/working\"\n",
    "\n",
    "merged_file_path = os.path.join(\"/kaggle/input/rag-dataset\", \"merged_data_updated.csv\")\n",
    "merged_df = pd.read_csv(merged_file_path)\n",
    "\n",
    "mask = merged_df[\"content\"] == \"No content available\"\n",
    "print(f\"🔎 需要重新爬取 {sum(mask)} 篇文章\")\n",
    "\n",
    "for idx in merged_df[mask].index:\n",
    "    url = merged_df.at[idx, \"url\"]\n",
    "    try:\n",
    "        article = Article(url)\n",
    "\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        content = article.text if article.text else \"No content available\"\n",
    "\n",
    "        if content != \"No content available\":\n",
    "            print(f\"✅ 成功爬取 {url}\")\n",
    "        else:\n",
    "            print(f\"⚠️ No content available agiain: {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fail to fetch content again {url}: {e}\")\n",
    "        content = \"Failed to fetch content.\"\n",
    "\n",
    "    merged_df.at[idx, \"content\"] = content\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "updated_file_path = os.path.join(\"/kaggle/working/\", \"merged_data_updated.csv\")\n",
    "merged_df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"🎉 更新后的数据已保存至: {updated_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T23:49:41.749889Z",
     "iopub.status.busy": "2025-02-10T23:49:41.749523Z",
     "iopub.status.idle": "2025-02-10T23:49:42.790872Z",
     "shell.execute_reply": "2025-02-10T23:49:42.789858Z",
     "shell.execute_reply.started": "2025-02-10T23:49:41.749857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "updated_file_path = os.path.join(\"/kaggle/working/\", \"merged_data_updated.csv\")\n",
    "merged_df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"🎉 更新后的数据已保存至: {updated_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T03:02:19.426397Z",
     "iopub.status.busy": "2025-02-10T03:02:19.426032Z",
     "iopub.status.idle": "2025-02-10T03:05:23.015835Z",
     "shell.execute_reply": "2025-02-10T03:05:23.014683Z",
     "shell.execute_reply.started": "2025-02-10T03:02:19.426368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from newsplease import NewsPlease\n",
    "import time\n",
    "import random\n",
    "from scrapy.settings import Settings\n",
    "\n",
    "custom_settings = Settings()\n",
    "custom_settings.set(\"DEFAULT_REQUEST_HEADERS\", {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36\"\n",
    "})\n",
    "\n",
    "folder_path = \"/kaggle/working\"\n",
    "\n",
    "merged_file_path = os.path.join(\"/kaggle/working\", \"merged_data_updated.csv\")\n",
    "merged_df = pd.read_csv(merged_file_path)\n",
    "\n",
    "mask = merged_df[\"content\"] == \"No content available\"\n",
    "print(sum(mask))\n",
    "\n",
    "for idx in merged_df[mask].index:\n",
    "    url = merged_df.at[idx, \"url\"]\n",
    "    try:\n",
    "        parsed_article = NewsPlease.from_url(url)\n",
    "        content = parsed_article.maintext if parsed_article else \"No content available\"\n",
    "        if content != \"No content available\":\n",
    "            print(f\"✅ Fetched content for {url}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Failed to fetch content again.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to fetch content from {url}: {e}\")\n",
    "        content = \"Failed to fetch content.\"\n",
    "    \n",
    "    merged_df.at[idx, \"content\"] = content\n",
    "    time.sleep(random.uniform(2, 6))\n",
    "\n",
    "updated_file_path = os.path.join(\"/kaggle/working/\", \"merged_data_updated.csv\")\n",
    "merged_df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"🎉 更新后的数据已保存至: {updated_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T02:41:23.795765Z",
     "iopub.status.busy": "2025-02-10T02:41:23.795431Z",
     "iopub.status.idle": "2025-02-10T02:41:24.054844Z",
     "shell.execute_reply": "2025-02-10T02:41:24.054052Z",
     "shell.execute_reply.started": "2025-02-10T02:41:23.795743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "updated_file_path = os.path.join(\"/kaggle/working/\", \"merged_data_updated.csv\")\n",
    "merged_df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"🎉 更新后的数据已保存至: {updated_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:11:54.575029Z",
     "iopub.status.busy": "2025-02-11T04:11:54.574551Z",
     "iopub.status.idle": "2025-02-11T04:11:54.598655Z",
     "shell.execute_reply": "2025-02-11T04:11:54.597141Z",
     "shell.execute_reply.started": "2025-02-11T04:11:54.574994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = os.path.join(\"/kaggle/input/rag-dataset\", \"merged_data_updated.csv\")\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T02:42:48.596333Z",
     "iopub.status.busy": "2025-02-11T02:42:48.595991Z",
     "iopub.status.idle": "2025-02-11T02:42:48.615500Z",
     "shell.execute_reply": "2025-02-11T02:42:48.614450Z",
     "shell.execute_reply.started": "2025-02-11T02:42:48.596310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Drop error pages\n",
    "print(\"Error page count: \", len(df[df[\"content\"] == \"Failed to fetch content.\"]))\n",
    "\n",
    "error_indices = df[df[\"content\"] == \"Failed to fetch content.\"].index\n",
    "df.drop(error_indices, inplace=True)\n",
    "\n",
    "print(\"Remaining page count: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T02:49:47.170829Z",
     "iopub.status.busy": "2025-02-11T02:49:47.170581Z",
     "iopub.status.idle": "2025-02-11T02:49:47.189887Z",
     "shell.execute_reply": "2025-02-11T02:49:47.189080Z",
     "shell.execute_reply.started": "2025-02-11T02:49:47.170810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Low Quality Content\n",
    "\n",
    "# Print rows where the title is \"Entertainment Events Today\"\n",
    "entertainment_df = df[df[\"title\"] == \"Entertainment Events Today\"]\n",
    "print(entertainment_df[\"content\"].reset_index(drop=True)[1])\n",
    "display(entertainment_df)\n",
    "\n",
    "error_indices = df[df[\"title\"] == \"Entertainment Events Today\"].index\n",
    "df.drop(error_indices, inplace=True)\n",
    "\n",
    "print(\"Remaining page count: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T02:52:55.994512Z",
     "iopub.status.busy": "2025-02-11T02:52:55.994234Z",
     "iopub.status.idle": "2025-02-11T02:52:56.014884Z",
     "shell.execute_reply": "2025-02-11T02:52:56.013929Z",
     "shell.execute_reply.started": "2025-02-11T02:52:55.994492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows based on the 'content' column\n",
    "\n",
    "# Identify and display rows that have duplicate content (showing all duplicates)\n",
    "duplicate_df = df[df.duplicated(subset=\"content\", keep=False)]\n",
    "print(\"Rows with duplicate 'content':\")\n",
    "display(duplicate_df)\n",
    "\n",
    "# Get the indices of duplicate rows, keeping the first occurrence of each unique 'content'\n",
    "duplicate_indices = df[df.duplicated(subset=\"content\", keep=\"first\")].index\n",
    "\n",
    "# Drop the duplicate rows from the DataFrame\n",
    "df.drop(duplicate_indices, inplace=True)\n",
    "\n",
    "# Print the remaining page count after removing duplicates\n",
    "print(\"Remaining page count: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T02:56:21.089013Z",
     "iopub.status.busy": "2025-02-11T02:56:21.088693Z",
     "iopub.status.idle": "2025-02-11T02:56:21.103689Z",
     "shell.execute_reply": "2025-02-11T02:56:21.102886Z",
     "shell.execute_reply.started": "2025-02-11T02:56:21.088988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values in critical fields \"content\"\n",
    "critical_fields = [\"content\"]\n",
    "\n",
    "# Identify rows with missing values in any of the critical fields\n",
    "missing_df = df[df[critical_fields].isnull().any(axis=1)]\n",
    "print(\"Rows with missing critical fields:\")\n",
    "display(missing_df)\n",
    "\n",
    "# Drop rows with missing critical fields\n",
    "missing_indices = missing_df.index\n",
    "df.drop(missing_indices, inplace=True)\n",
    "\n",
    "print(\"Remaining page count after removing rows with missing critical fields:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T02:57:19.236606Z",
     "iopub.status.busy": "2025-02-11T02:57:19.236330Z",
     "iopub.status.idle": "2025-02-11T02:57:19.478198Z",
     "shell.execute_reply": "2025-02-11T02:57:19.477416Z",
     "shell.execute_reply.started": "2025-02-11T02:57:19.236585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check the average word count in the 'content' field of the DataFrame\n",
    "df[\"word_count\"] = df[\"content\"].apply(lambda content: len(content.split()))\n",
    "average_word_count = df[\"word_count\"].mean()\n",
    "\n",
    "print(\"Average word count:\", average_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T02:58:02.447860Z",
     "iopub.status.busy": "2025-02-11T02:58:02.447587Z",
     "iopub.status.idle": "2025-02-11T02:58:02.937799Z",
     "shell.execute_reply": "2025-02-11T02:58:02.936996Z",
     "shell.execute_reply.started": "2025-02-11T02:58:02.447836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a CSV file without the index column.\n",
    "df.to_csv(\"nytimes_cleaned_data.csv\", index=False)\n",
    "print(\"DataFrame has been saved as 'nytimes_cleaned_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futher Insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:45:13.872109Z",
     "iopub.status.busy": "2025-02-11T04:45:13.871751Z",
     "iopub.status.idle": "2025-02-11T04:45:14.613850Z",
     "shell.execute_reply": "2025-02-11T04:45:14.612621Z",
     "shell.execute_reply.started": "2025-02-11T04:45:13.872082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = \"/kaggle/input/rag-dataset/nytimes_cleaned_data_2020.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()\n",
    "\n",
    "for i in range(3):\n",
    "    print(df[\"content\"][i])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:45:17.044260Z",
     "iopub.status.busy": "2025-02-11T04:45:17.043903Z",
     "iopub.status.idle": "2025-02-11T04:45:17.075260Z",
     "shell.execute_reply": "2025-02-11T04:45:17.073868Z",
     "shell.execute_reply.started": "2025-02-11T04:45:17.044234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 定义要删除的固定文本\n",
    "extra_text = (\n",
    "    \"Thank you for your patience while we verify access. If you are in Reader mode please exit and log into your Times account, or subscribe for all of The Times.\\n\"\n",
    "    \"Thank you for your patience while we verify access.\\n\"\n",
    "    \"Already a subscriber? Log in.\\n\"\n",
    "    \"Want all of The Times? Subscribe.\"\n",
    ")\n",
    "\n",
    "# 定义处理函数\n",
    "def remove_extra_text(content):\n",
    "    if extra_text in content:\n",
    "        return content.split(extra_text)[0]\n",
    "    return content\n",
    "\n",
    "# 对 DataFrame 的每个 \"content\" 应用该函数\n",
    "df[\"content\"] = df[\"content\"].apply(remove_extra_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:45:17.866479Z",
     "iopub.status.busy": "2025-02-11T04:45:17.866129Z",
     "iopub.status.idle": "2025-02-11T04:45:17.876813Z",
     "shell.execute_reply": "2025-02-11T04:45:17.875627Z",
     "shell.execute_reply.started": "2025-02-11T04:45:17.866451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = 3\n",
    "\n",
    "for i in range(n):\n",
    "    idx = random.randint(0, len(df) - 1)\n",
    "    print(\"Title:\", df[\"title\"][idx])\n",
    "    print(df[\"content\"][idx])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:45:22.743283Z",
     "iopub.status.busy": "2025-02-11T04:45:22.742874Z",
     "iopub.status.idle": "2025-02-11T04:45:22.775289Z",
     "shell.execute_reply": "2025-02-11T04:45:22.774202Z",
     "shell.execute_reply.started": "2025-02-11T04:45:22.743250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 定义一个函数，从 URL 中提取日期字符串（格式：YYYY-MM-DD）\n",
    "def extract_date(url):\n",
    "    # 匹配模式：/年份/月份/日期/，例如 /2025/01/28/\n",
    "    pattern = r\"/(\\d{4})/(\\d{2})/(\\d{2})/\"\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        year, month, day = match.groups()\n",
    "        return f\"{year}-{month}-{day}\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 对 DataFrame 的 url 列应用该函数，生成一个新的 date 列\n",
    "df[\"date\"] = df[\"url\"].apply(extract_date)\n",
    "\n",
    "# 如果需要将 date 列转换为 datetime 类型，可以使用：\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:47:41.667037Z",
     "iopub.status.busy": "2025-02-11T04:47:41.666556Z",
     "iopub.status.idle": "2025-02-11T04:47:41.676586Z",
     "shell.execute_reply": "2025-02-11T04:47:41.675197Z",
     "shell.execute_reply.started": "2025-02-11T04:47:41.667002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df[\"section\"].unique())\n",
    "print(df[\"section\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:45:25.866577Z",
     "iopub.status.busy": "2025-02-11T04:45:25.866209Z",
     "iopub.status.idle": "2025-02-11T04:45:25.894690Z",
     "shell.execute_reply": "2025-02-11T04:45:25.893445Z",
     "shell.execute_reply.started": "2025-02-11T04:45:25.866511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# drop data before 2020\n",
    "\n",
    "# 生成 date 列并转换为 datetime 类型\n",
    "df[\"date\"] = df[\"url\"].apply(extract_date)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# 删除 2020 年以前的数据\n",
    "df = df[df[\"date\"].dt.year >= 2020]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:45:27.768784Z",
     "iopub.status.busy": "2025-02-11T04:45:27.768374Z",
     "iopub.status.idle": "2025-02-11T04:45:28.101186Z",
     "shell.execute_reply": "2025-02-11T04:45:28.100013Z",
     "shell.execute_reply.started": "2025-02-11T04:45:27.768746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# 使用 plt.hist 绘制直方图，自动将日期数据分成若干个 bin\n",
    "plt.hist(df[\"date\"], bins=25, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Date Distribution\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:45:34.944727Z",
     "iopub.status.busy": "2025-02-11T04:45:34.944296Z",
     "iopub.status.idle": "2025-02-11T04:45:35.680739Z",
     "shell.execute_reply": "2025-02-11T04:45:35.679576Z",
     "shell.execute_reply.started": "2025-02-11T04:45:34.944691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a CSV file without the index column.\n",
    "df.to_csv(\"nytimes_cleaned_data_2020.csv\", index=False)\n",
    "print(\"DataFrame has been saved as 'nytimes_cleaned_data_2020.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check `the guardian` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:45:35.682605Z",
     "iopub.status.busy": "2025-02-11T04:45:35.682224Z",
     "iopub.status.idle": "2025-02-11T04:45:36.102350Z",
     "shell.execute_reply": "2025-02-11T04:45:36.101193Z",
     "shell.execute_reply.started": "2025-02-11T04:45:35.682574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = \"/kaggle/input/si630-ai-generated-news-detection/train_news_real_df.csv\"\n",
    "guadian_df = pd.read_csv(path)\n",
    "guadian_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:45:36.859711Z",
     "iopub.status.busy": "2025-02-11T04:45:36.859304Z",
     "iopub.status.idle": "2025-02-11T04:45:37.561158Z",
     "shell.execute_reply": "2025-02-11T04:45:37.560106Z",
     "shell.execute_reply.started": "2025-02-11T04:45:36.859675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_date_from_passage_id(passage_id):\n",
    "    \"\"\"\n",
    "    从 passage_id 中提取日期，假设格式类似于:\n",
    "    \"education/2025/jan/31/some-title...\"\n",
    "    \"\"\"\n",
    "    # 正则表达式匹配：4位年份/3位月份（字母）/2位日期\n",
    "    match = re.search(r'(\\d{4})/([a-z]{3})/(\\d{2})', passage_id, re.IGNORECASE)\n",
    "    if match:\n",
    "        year, month, day = match.groups()\n",
    "        # 将月份转为首字母大写（例如 \"jan\" -> \"Jan\"），以便于日期转换\n",
    "        month = month.capitalize()\n",
    "        date_str = f\"{year}/{month}/{day}\"\n",
    "        try:\n",
    "            date = pd.to_datetime(date_str, format=\"%Y/%b/%d\")\n",
    "            return date\n",
    "        except Exception as e:\n",
    "            return pd.NaT\n",
    "    else:\n",
    "        return pd.NaT\n",
    "\n",
    "# 对 DataFrame 的 passage_id 列应用该函数，生成新的 date 列\n",
    "guadian_df[\"date\"] = guadian_df[\"passage_id\"].apply(extract_date_from_passage_id)\n",
    "\n",
    "# 检查结果\n",
    "display(guadian_df[[\"passage_id\", \"date\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:48:10.046958Z",
     "iopub.status.busy": "2025-02-11T04:48:10.046571Z",
     "iopub.status.idle": "2025-02-11T04:48:10.054323Z",
     "shell.execute_reply": "2025-02-11T04:48:10.053030Z",
     "shell.execute_reply.started": "2025-02-11T04:48:10.046929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(guadian_df[\"section\"].unique())\n",
    "print(guadian_df[\"section\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:45:39.048599Z",
     "iopub.status.busy": "2025-02-11T04:45:39.048235Z",
     "iopub.status.idle": "2025-02-11T04:45:39.442321Z",
     "shell.execute_reply": "2025-02-11T04:45:39.441202Z",
     "shell.execute_reply.started": "2025-02-11T04:45:39.048569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# 使用 plt.hist 绘制直方图，自动将日期数据分成若干个 bin\n",
    "plt.hist(guadian_df[\"date\"], bins=25, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Date Distribution\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6588192,
     "sourceId": 10671971,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6631713,
     "sourceId": 10718390,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
